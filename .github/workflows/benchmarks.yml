# Run performance benchmarks
#
# Continuous benchmarking using pytest-codspeed. Measures the execution speed
# of tests marked with @pytest.mark.benchmark decorator.

name: Benchmarks

on:
  # Run on pushes to the main branch
  push:
    branches: [ main ]
  # Uncomment the 'pull_request' line below to trigger the workflow in PR
  pull_request:
  # `workflow_dispatch` allows CodSpeed to trigger backtest
  # performance analysis in order to generate initial data.
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

jobs:
  benchmarks:
    runs-on: ubuntu-22.04
    defaults:
      run:
        shell: bash -l {0}

    steps:
      # Checkout current git repository
      - name: Checkout
        uses: actions/checkout@v4.1.1
        with:
          # fetch all history so that setuptools-scm works
          fetch-depth: 0

      # Install Micromamba with conda-forge dependencies
      - name: Setup Micromamba
        uses: mamba-org/setup-micromamba@v1.7.3
        with:
          environment-name: pygmt
          condarc: |
            channels:
              - conda-forge
              - nodefaults
          cache-downloads: true
          cache-environment: true
          create-args: >-
            python=3.12
            gmt=6.4.0

      # Show installed pkg information for postmortem diagnostic
      - name: List installed packages
        run: micromamba list

      # Run the benchmark tests
      - name: Run benchmarks
        uses: CodSpeedHQ/action@v2.0.2
        with:
          run: |
            python -m pip install -U numpy pandas xarray netCDF4 packaging \
                                  pip pytest pytest-benchmark pytest-codspeed \
                                  setuptools
            python -m pip list
            make install
            which gmt
            make test PYTEST_EXTRA="-r P --codspeed"
        env:
          GMT_LIBRARY_PATH: /home/runner/micromamba/envs/pygmt/lib/
