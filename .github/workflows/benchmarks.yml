# Run performance benchmarks
#
# Continuous benchmarking using pytest-codspeed. Measures the execution speed
# of tests marked with @pytest.mark.benchmark decorator.

name: Benchmarks

on:
  # Run on pushes to the main branch
  push:
    branches: [ main ]
  # Uncomment the 'pull_request' line below to trigger the workflow in PR
  pull_request:
  # `workflow_dispatch` allows CodSpeed to trigger backtest
  # performance analysis in order to generate initial data.
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

jobs:
  benchmarks:
    runs-on: ubuntu-22.04
    defaults:
      run:
        shell: bash -l {0}

    steps:
      # Checkout current git repository
      - name: Checkout
        uses: actions/checkout@v4.1.1
        with:
          # fetch all history so that setuptools-scm works
          fetch-depth: 0

      # Install Miniconda with conda-forge dependencies
      - name: Setup Miniconda
        uses: conda-incubator/setup-miniconda@v3.0.1
        with:
          activate-environment: pygmt
          python-version: '3.12'
          channels: conda-forge,nodefaults
          channel-priority: strict

      # Install GMT and dependencies from conda-forge
      - name: Install dependencies
        run: |
          conda install --solver=libmamba gmt=6.4.0 \
                        numpy pandas xarray netCDF4 packaging \
                        pytest pytest-benchmark pytest-codspeed setuptools

      # Install dependencies from PyPI and run the benchmark tests
      - name: Run benchmarks
        uses: CodSpeedHQ/action@v2.0.2
        with:
          run: |
            ls -lh /usr/share/miniconda/envs/pygmt/lib/
            make install
            make test PYTEST_EXTRA="-r P --codspeed"
        env:
          GMT_LIBRARY_PATH: /usr/share/miniconda/envs/pygmt/lib/
